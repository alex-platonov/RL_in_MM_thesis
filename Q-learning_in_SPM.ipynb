{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPM (Simple Probabilistic Model)\n",
    "\n",
    "The simple probabilistic model (SPM) is the first to be used. \n",
    "> * The time _t_ can take integer values between _0_ and _T_.\n",
    ">\n",
    "> * The midprice _S<sub>t</sub>_ is a Brownian motion rounded to the closest tick.\n",
    ">\n",
    "> * The market maker has to quote bid and ask prices every second.\n",
    ">\n",
    "> * The market maker can put the bid and ask depths at _d_ different levels, from _0_ to _d - 1_ ticks away from the mid price.\n",
    ">\n",
    "> * The cash process _X<sub>t</sub>_ denotes the market maker's cash at time _t_.\n",
    ">\n",
    "> * The inventory process _Q<sub>t</sub>_ denotes the market maker's inventory at time _t_.\n",
    ">\n",
    "> * The value process _V<sub>t</sub>_ denotes the value of the market maker's position at time _t_, that is its cash plus the value of its current inventory.\n",
    ">\n",
    "> * The market maker can see the current time and its inventory _(t,Q<sub>t</sub>)_ before taking an action.\n",
    ">\n",
    "> * At time _t = T_ the market maker is forced to liquidate its position.\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of 1AAPL.\n",
    "\n",
    "Based on Cartea et al.'s definition, an analytically optimal strategy can be defined, that is used as a benchmark for the strategies derived with Q-learning. However, there is no guarantee that these strategies are optimal in the discretized version of the model.\n",
    "\n",
    "An example of the optimal bid depths for a specific set of model parameters is shown in the figure below. **Note** that these depths are _not_ discretized in terms of depth, that is they're not rounded to the closest tick.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/ContinuousBid30.png\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-learning\n",
    "\n",
    "Importing the source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Q-learning file for the SPM\n",
    "from simple_model_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters to be used for the environment and the hyperparameters for the Q-learning.\n",
    "\n",
    "The model has an episode length of *T = 20* and a running inventory penalty of *$\\phi$ = 10<sup>-4</sup>*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"d\": 4,         # the number of different quotation depths\n",
    "                \"T\": 20,        # the length of the episode\n",
    "                \"dp\": 0.01,     # the tick size\n",
    "                \"min_dp\": 0,    # the minimum number of ticks from the mid price that is allowed to put prices at\n",
    "                \"phi\": 1e-4     # the running inventory penalty\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify the hyperparameter values.\n",
    "\n",
    "The set of choices is relatively limited. The required selections concern the parameter schedules for the Ïµ-greedy policy, the learning-rate schedule, and whether exploring starts are enabled. In addition, the training and evaluation budgets must be fixed: the training horizon (number of episodes), the number of independent training runs, and the length of the evaluation phase.\n",
    "\n",
    "> *\\_start* denotes the initial value of a parameter;\n",
    "> \n",
    "> *\\_end* denotes its terminal value;\n",
    "> \n",
    "> *\\_cutoff* specifies the fraction of training at which the terminal value is reached (e.g., 0.5 corresponds to 50% of the training episodes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning_params = {\n",
    "        # epsilon-greedy values (linear decay)\n",
    "        \"epsilon_start\": 1,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_cutoff\": 0.5,\n",
    "\n",
    "        # learning-rate values (exponential decay)\n",
    "        \"alpha_start\": 0.5,\n",
    "        \"alpha_end\": 0.001,\n",
    "        \"alpha_cutoff\": None,\n",
    "\n",
    "        # exploring starts values (linear decay)\n",
    "        \"beta_start\": 1,\n",
    "        \"beta_end\": 0.05,\n",
    "        \"beta_cutoff\": 0.5,\n",
    "        \"exploring_starts\": True\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "        \"n_train\" : 1e5,\n",
    "        \"n_test\" : 1e4,\n",
    "        \"n_runs\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the the results dir\n",
    "folder_mode = True\n",
    "folder_name = \"spm_example\"\n",
    "save_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function *Q\\_learning\\_comparison* to run the qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 1 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:15.900000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:31.910000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:55.660000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:26.240000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:06.430000\n",
      "0:06:19.300000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 2 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:24.510000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:40.060000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:01:01.650000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:29.790000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:26.520000\n",
      "0:04:53.050000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 3 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:18.700000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:35.560000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:58.220000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:27.570000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:13.260000\n",
      "0:02:13.260000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 4 IN PROGRESS...\n",
      "\tEpisode 20000 (20%), 0:02:15.330000 remaining of this run\n",
      "\tEpisode 40000 (40%), 0:01:32.920000 remaining of this run\n",
      "\tEpisode 60000 (60%), 0:00:56.140000 remaining of this run\n",
      "\tEpisode 80000 (80%), 0:00:26.560000 remaining of this run\n",
      "\tEpisode 100000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER spm_example ALREADY EXISTS\n",
      "...FINISHED IN 0:02:07.830000\n",
      "========================================\n",
      "FULL TRAINING COMPLETED IN 0:08:54.050000\n"
     ]
    }
   ],
   "source": [
    "Q_learning_comparison(\n",
    "    **hyperparams,\n",
    "    args                = model_params,\n",
    "    Q_learning_args     = Q_learning_params,\n",
    "    folder_mode         = folder_mode,\n",
    "    folder_name         = folder_name,\n",
    "    save_mode           = save_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the strategies\n",
    "\n",
    "Some graphical representations of the results obtained while running *Q\\_learning\\_comparison*.\n",
    "\n",
    "1) The reward and the state-value at (0,0) during training.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/simple_model/spm_example/results_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "Here it looks like that the Q-learning has converged, however, it has not. It has to be trained for longer, which will be evident in the future.\n",
    "\n",
    "The figure below shows the learnt bid depths.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/simple_model/spm_example/opt_bid_strategy.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The average rewards of the Q-learning strategies versus benchmarking strategies are compare below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/simple_model/spm_example/box_plot_benchmarking.png\"/>\n",
    "</div>\n",
    "\n",
    "Further results below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy                 mean reward    std reward\n",
      "---------------------  -------------  ------------\n",
      "analytical_discrete        0.127752      0.0764701\n",
      "analytical_continuous      0.132987      0.0717148\n",
      "constant (d=2)             0.0987989     0.0796333\n",
      "random                     0.0618333     0.0958115\n",
      "Q_learning (best run)      0.123278      0.0772611\n",
      "Q_learning (average)       0.127178      0.0785348\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results/simple_model/spm_example/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all it looks like the Q-learning has been able to find decent strategies, but it needs to train for longer in order to find strategies that equal the analytical strategies in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More results?\n",
    "\n",
    "There are a lot more figures and tables to explore which can be found in the **[spm_example](https://github.com/KodAgge/Reinforcement-Learning-for-Market-Making/tree/main/code/results/simple_model/spm_example)** folder."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
