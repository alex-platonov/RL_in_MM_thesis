{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MC Model\n",
    "\n",
    "The Markov chain (MC) model is the second model used in this thesis. It is significantly more complex than the simple probabilistic model, however, it is still quite rudimentary in comparison with real-life markets. \n",
    "\n",
    "In this model, the limit order book (LOB) is modelled explicitly. There are six event types:\n",
    "\n",
    "> 1. Buy limit orders \n",
    "> 2. Sell limit orders\n",
    "> 3. Cancel buy orders\n",
    "> 4. Cancel sell orders\n",
    "> 5. Buy market orders\n",
    "> 6. Sell market orders\n",
    "\n",
    "The arrival of an order results in a state transition in the Markov chain. An example of how the arrival of different orders affect the LOB is shown in the image below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LOBDynamics.png\" width=800/>\n",
    "</div>\n",
    "\n",
    "\n",
    "As in the simple probabilistic model, the following assumptions and definitions are retained:\n",
    "\n",
    "> - Time \\(t\\) takes integer values from \\(0\\) to \\(T\\).\n",
    "> - Bid and ask quotes are updated once per second.\n",
    "> - Bid and ask depths are chosen from *max_quote_depth* discrete levels, corresponding to \\(1\\) through *max_quote_depth* ticks away from the best bid and best ask, respectively.\n",
    "> - The cash process \\(X_t\\) denotes the market maker’s cash position at time \\(t\\).\n",
    "> - The inventory process \\(Q_t\\) denotes the market maker’s inventory at time \\(t\\).\n",
    "> - The value process \\(V_t\\) denotes the marked-to-liquidation value of the market maker’s position at time \\(t\\), i.e., cash plus the liquidation value of the current inventory.\n",
    "> - The state available for decision-making includes the current time \\(t\\) and the current inventory. Inventory \\(Q_t\\) is discretised into bins with width determined by the parameter \\(\\kappa\\).\n",
    "> - At \\(t = T\\), the position is liquidated compulsorily.\n",
    "\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of 1AAPL.\n",
    "\n",
    "Contrary to the SPM, it is not possible to derive an analytically optimal strategy in the MC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-learning\n",
    "\n",
    "After that short introduction, it's time for some reinforcement learning in the form of Q-learning.\n",
    "\n",
    "We start by importing the needed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import the Q-learning file for the markov chain model\n",
    "from mc_model_evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment parameters and the Q-learning hyperparameters must next be specified.\n",
    "\n",
    "The MC model introduces several additional environment parameters, as outlined in the code snippet below. A key choice is the use of a longer episode (trading window), with *T=100*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"dt\": 1,                    # the length of the time steps\n",
    "                \"T\": 100,                   # the length of the episode\n",
    "                \"num_time_buckets\": 100,    # how many bins that should be used for the time\n",
    "                \"kappa\": 3,                 # the size of the inventory bins\n",
    "                \"num_levels\": 10,           # how many depth levels that should be included in the LOB\n",
    "                \"default_order_size\": 5,    # the size of the orders the MM places\n",
    "                \"max_quote_depth\": 5,       # how deep the MM can put its quotes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter values must next be specified. The set of choices is relatively limited: parameter schedules are selected for the ϵ-greedy exploration policy and for the learning rate. In addition, the experimental budget is fixed by choosing the training horizon (number of episodes), the number of independent training runs, and the evaluation horizon.\n",
    "\n",
    "> *\\_start* denotes the initial value of a parameter;\n",
    ">\n",
    "> *\\_end* denotes its terminal value;\n",
    ">\n",
    "> *\\_cutoff* specifies the fraction of training at which the terminal value is reached (e.g., 0.5 corresponds to 50% of the training episodes);\n",
    "\n",
    "**Note:** exploring starts in not used this setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_learning_params = {\n",
    "        # epsilon-greedy values (linear decay)\n",
    "        \"epsilon_start\": 1,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_cutoff\": 0.5,\n",
    "\n",
    "        # learning-rate values (exponential decay)\n",
    "        \"alpha_start\": 0.5,\n",
    "        \"alpha_end\": 0.001,\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "        \"n_train\" : 3e3,\n",
    "        \"n_test\" : 3e2,\n",
    "        \"n_runs\" : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decide where to save our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the results folder\n",
    "folder_mode = True\n",
    "folder_name = \"mc_example\"\n",
    "save_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning with *Q\\_learning\\_comparison*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 1 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:04:02.470000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:03:00.580000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:59.500000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:59.630000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:58.260000\n",
      "0:14:54.790000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 2 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:03:58.930000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:02:58.210000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:56.890000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:58.010000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:47.830000\n",
      "0:09:35.650000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 3 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:03:52.790000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:02:56.420000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:59.340000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:59.180000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:56.080000\n",
      "0:04:56.080000 REMAINING OF THE TRAINING\n",
      "========================================\n",
      "RUN 4 IN PROGRESS...\n",
      "\tEpisode 600 (20%), 0:04:07.460000 remaining of this run\n",
      "\tEpisode 1200 (40%), 0:03:01.970000 remaining of this run\n",
      "\tEpisode 1800 (60%), 0:01:58.840000 remaining of this run\n",
      "\tEpisode 2400 (80%), 0:00:58.820000 remaining of this run\n",
      "\tEpisode 3000 (100%), 0:00:00 remaining of this run\n",
      "THE FOLDER mc_example ALREADY EXISTS\n",
      "...FINISHED IN 0:04:51.770000\n",
      "========================================\n",
      "FULL TRAINING COMPLETED IN 0:19:33.930000\n",
      "\n",
      "PLOTTING REWARDS... DONE\n",
      "\n",
      "EVALUATING DIFFERENT Q-STRATEGIES.... DONE\n",
      "\n",
      "EVALUATING DIFFERENT STRATEGIES.... DONE\n",
      "\n",
      "SHOWING STRATEGIES... DONE\n",
      "\n",
      "SHOWING STD FOR Q MATRIX\n",
      "\n",
      "HEATMAP FOR ERRORS\n"
     ]
    }
   ],
   "source": [
    "Q_learning_comparison(\n",
    "    **hyperparams,\n",
    "    args=model_params,\n",
    "    Q_learning_args=Q_learning_params,\n",
    "    folder_mode = folder_mode,\n",
    "    folder_name = folder_name,\n",
    "    save_mode = save_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the strategies\n",
    "\n",
    "Below are some figures resulting from running *Q\\_learning\\_comparison*.\n",
    "\n",
    "The reward and the state-value at (0,0) during training:\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model/mc_example/results_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "Here it looks like that the Q-learning has converged, however, it has not. It has to be trained for *much* longer. This will become very evident further.\n",
    "\n",
    "The figure below shows the learnt bid depths.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model/mc_example/opt_bid_heat.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "he average rewards of the Q-learning strategies versus some benchmarking strategiesare compared below:\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model/mc_example/box_plot_benchmarking.png\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy                 mean reward    std reward\n",
      "---------------------  -------------  ------------\n",
      "constant (d=1)              1.77           8.81346\n",
      "random                     -0.423333       7.72123\n",
      "Q_learning (best run)      -0.48           5.63941\n",
      "Q_learning (average)       -0.79           5.94019\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results/mc_model/mc_example/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that substantially longer training is required before any effective strategies emerge. Notably, the constant-depth baseline markedly outperforms all other strategies."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
