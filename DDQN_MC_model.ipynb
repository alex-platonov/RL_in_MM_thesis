{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MC Model\n",
    "\n",
    "The Markov chain (MC) model is the second model used in this thesis. It is significantly more complex than the simple probabilistic model, however, it is still quite rudimentary in comparison with real-life markets. \n",
    "\n",
    "In this model, the limit order book (LOB) is modelled explicitly. There are six event types:\n",
    "\n",
    "> 1. Buy limit orders \n",
    "> 2. Sell limit orders\n",
    "> 3. Cancel buy orders\n",
    "> 4. Cancel sell orders\n",
    "> 5. Buy market orders\n",
    "> 6. Sell market orders\n",
    "\n",
    "The arrival of an order triggers a state transition in the Markov chain. An illustration of how different order arrivals modify the limit order book is provided in the figure below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/LOBDynamics.png\" width=800/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Same as in SPM there is:\n",
    "\n",
    "> * The time _t_ can take integer values between _0_ and _T_.\n",
    ">\n",
    "> * The market maker has to quote bid and ask prices every second.\n",
    ">\n",
    "> * The market maker can put the bid and ask depths at *max\\_quote\\_depth* different levels, from _1_ to *max\\_quote\\_depth* ticks away from the best ask and best bid price respectively.\n",
    ">\n",
    "> * The cash process _X<sub>t</sub>_ denotes the market makers cash at time _t_.\n",
    ">\n",
    "> * The inventory process _Q<sub>t</sub>_ denotes the market makers inventory at time _t_.\n",
    ">\n",
    "> * The value process _V<sub>t</sub>_ denotes the value of the market maker's position at time _t_, that is its cash plus the value of its current inventory.\n",
    ">\n",
    "> * The market maker can see the current time _t_ , its inventory _Q<sub>t</sub>_, the spread and the *full LOB* before taking an action.\n",
    ">\n",
    "> * At time _t = T_ the market maker is forced to liquidate its position.\n",
    "\n",
    "The _tick_ is the smallest tradeable unit of the underlying, for instance $0.01 of 1AAPL.\n",
    "\n",
    "Contrary to the SPM, it is not possible to derive an analytically optimal strategy in the MC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRL stage\n",
    "1) Import the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from mc_model_mm_deep_rl_batch import (\n",
    "    train_multiple_agents_batch,\n",
    "    evaluate_DDQN_batch\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment parameters and the DDQN hyperparameters must first be specified.\n",
    "\n",
    "The Markov-chain model introduces several additional environment parameters; these are summarised in the code snippet below. A key design choice is the use of a longer episode (trading window), with *(T = 100)*.\n",
    "\n",
    "DDQN adds further choices due to the neural-network function approximator. In particular, the reward is rescaled to help keep it within *([-1,1])* for training stability. Moreover, because the agent observes the full limit order book, the initial book state is randomised at the start of each episode to increase state coverage during learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "                \"dt\": 1,                    # the length of the time steps\n",
    "                \"T\": 100,                   # the length of the episode\n",
    "                \"num_levels\": 10,           # number of depth levels to be included in the LOB\n",
    "                \"default_order_size\": 5,    # the size of the orders the MM places\n",
    "                \"max_quote_depth\": 5,       # how deep the MM can put its quotes\n",
    "                \"reward_scale\": 0.1,        # a factor all rewards will be multiplied with\n",
    "                \"randomize_reset\": True     # a random LOB state is chosen at the start of every episode\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to select the hyperparameter values.\n",
    "\n",
    "DDQN introduces a substantial number of tunable settings. In brief, these choices cover the neural-network architecture, experience replay configuration, and the parameters governing the Ïµ-greedy exploration policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "                \"n_train\": int(2e5),    # the number of steps the agents will be trained for\n",
    "                \"n_test\": int(1e2),     # the number of episodes the agents will be evaluated for\n",
    "                \"n_runs\": 4             # the number of agents that will be trained\n",
    "}\n",
    "\n",
    "DDQN_params = {\n",
    "                # network params\n",
    "                \"hidden_size\": 64,                                          # the hidden size of the network\n",
    "                \"buffer_size\": hyperparams[\"n_train\"] / 200,                # the size of the experience replay bank\n",
    "                \"replay_start_size\": hyperparams[\"n_train\"] / 200,          # after how many number of steps the experience replay is started\n",
    "                \"target_update_interval\": hyperparams[\"n_train\"] / 100,     # how often the target network is updated\n",
    "                \"update_interval\": 2,                                       # how often the online network is updated\n",
    "                \"minibatch_size\": 16,                                       # the size of the minibatches used\n",
    "\n",
    "                # epsilon greedy (linear decay)\n",
    "                \"exploration_initial_eps\": 1,                               # the starting value of the exploration rate\n",
    "                \"exploration_final_eps\": 0.05,                              # the final value of the exploration rate\n",
    "                \"exploration_fraction\": 0.5,                                # when the final value is reached\n",
    "\n",
    "                # learning rate\n",
    "                \"learning_rate_dqn\": 1e-4,                                  # the learning rate used (Adam)\n",
    "                \n",
    "                # other params\n",
    "                \"num_envs\": 10,                                             # how many parallelized environments\n",
    "                \"n_train\": hyperparams[\"n_train\"], \n",
    "                \"n_runs\": hyperparams[\"n_runs\"],\n",
    "                \"reward_scale\": model_params[\"reward_scale\"],\n",
    "\n",
    "                # logging params\n",
    "                \"log_interval\": hyperparams[\"n_train\"] / 100,               # the frequency of saving information\n",
    "                \"num_estimate\": 10000,                                      # how many states that should be used for estimating q_values\n",
    "                \"n_states\": 10                                              # the number of states heatmaps are averaged over\n",
    "                \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model it is the emulating of the market that is the bottleneck, so it runs faster on a cpu than a gpu. This holds even when multithreading is used for the emulation, which we use in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the dir with the results\n",
    "folder_name = \"mc_deep_example\"\n",
    "\n",
    "outdir = f\"results/mc_model_deep/{folder_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run DRL use the function *train\\_multiple\\_agents\\_batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mc_model_mm_deep_rl_batch:The folder results/mc_model_deep/mc_deep_example/estimate_folder already exists.\n",
      "INFO:mc_model_mm_deep_rl_batch:Run 1 in progress.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 40000 (20%), 0:05:15.470000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 80000 (40%), 0:04:12.870000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 120000 (60%), 0:02:55.090000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 160000 (80%), 0:01:28.160000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 200000 (100%), 0:00:00 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:Saved the agent to results/mc_model_deep/mc_deep_example/model_folder/200000_finish_1\n",
      "INFO:mc_model_mm_deep_rl_batch:Finished in 0:07:13.630000.\n",
      "INFO:mc_model_mm_deep_rl_batch:0:21:40.900000 remaining of the training.\n",
      "INFO:mc_model_mm_deep_rl_batch:Run 2 in progress.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 40000 (20%), 0:05:31.020000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 80000 (40%), 0:04:20.290000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 120000 (60%), 0:02:55.860000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 160000 (80%), 0:01:27.870000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 200000 (100%), 0:00:00 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:Saved the agent to results/mc_model_deep/mc_deep_example/model_folder/200000_finish_2\n",
      "INFO:mc_model_mm_deep_rl_batch:Finished in 0:07:17.930000.\n",
      "INFO:mc_model_mm_deep_rl_batch:0:14:35.850000 remaining of the training.\n",
      "INFO:mc_model_mm_deep_rl_batch:Run 3 in progress.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 40000 (20%), 0:05:32.020000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 80000 (40%), 0:04:21.020000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 120000 (60%), 0:02:55.980000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 160000 (80%), 0:01:29.170000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 200000 (100%), 0:00:00 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:Saved the agent to results/mc_model_deep/mc_deep_example/model_folder/200000_finish_3\n",
      "INFO:mc_model_mm_deep_rl_batch:Finished in 0:07:21.490000.\n",
      "INFO:mc_model_mm_deep_rl_batch:0:07:21.490000 remaining of the training.\n",
      "INFO:mc_model_mm_deep_rl_batch:Run 4 in progress.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 40000 (20%), 0:05:30.380000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 80000 (40%), 0:04:07.410000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 120000 (60%), 0:02:44.620000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 160000 (80%), 0:01:23.110000 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:- Step 200000 (100%), 0:00:00 remaining of the run.\n",
      "INFO:mc_model_mm_deep_rl_batch:Saved the agent to results/mc_model_deep/mc_deep_example/model_folder/200000_finish_4\n",
      "INFO:mc_model_mm_deep_rl_batch:Finished in 0:06:57.750000.\n"
     ]
    }
   ],
   "source": [
    "train_multiple_agents_batch(\n",
    "    DDQN_params, \n",
    "    model_params, \n",
    "    hyperparams[\"n_train\"], \n",
    "    outdir, \n",
    "    hyperparams[\"n_runs\"], \n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the strategies\n",
    "\n",
    "To evaluate the agents use the function *evaluate\\_DDQN\\_batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mc_model_mm_deep_rl_batch:The folder results/mc_model_deep/mc_deep_example/image_folder already exists.\n",
      "INFO:mc_model_mm_deep_rl_batch:Plotting training.\n",
      "INFO:mc_model_mm_deep_rl_batch:Plotting strategies.\n",
      "INFO:mc_model_mm_deep_rl_batch:Evaluating agents.\n",
      "INFO:mc_model_mm_deep_rl_batch:Evaluating benchmarks...\n",
      "INFO:mc_model_mm_deep_rl_batch:...best agent\n",
      "INFO:mc_model_mm_deep_rl_batch:...mean agent\n",
      "INFO:mc_model_mm_deep_rl_batch:...constant strategy\n",
      "INFO:mc_model_mm_deep_rl_batch:...random_strategy\n",
      "INFO:mc_model_mm_deep_rl_batch:Visualizing the strategies.\n",
      "INFO:mc_model_mm_deep_rl_batch:The folder results/mc_model_deep/mc_deep_example/image_folder already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_DDQN_batch(\n",
    "    outdir, \n",
    "    n_test=hyperparams[\"n_test\"],                  \n",
    "    Q=10,       # how many depths that should be displayed in the heatmaps\n",
    "    randomize_start=model_params[\"randomize_reset\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the results of *evaluate\\_DDQN\\_batch* see the figures below.\n",
    "\n",
    "The reward, the estimated state-value at (0,0) and the network loss during training:\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/training_graph.png\"/>\n",
    "</div>\n",
    "\n",
    "Here it looks like that the algorithm hasn't converged. Indeed, it has to be trained for much longer. It probably also needs hyperparameter tuning since the q-estimate and the loss seems to be diverging.\n",
    "\n",
    "The figure below shows the learnt bid depths:\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/bid_heat_randomized_10.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The average rewards of the Q-learning strategies versus some benchmarking strategies are displayed in the boxplot below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/box_plot_benchmarking.png\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy           mean reward    std reward    reward per action    reward per second\n",
      "---------------  -------------  ------------  -------------------  -------------------\n",
      "constant (d=1)          0.0402     0.104441              0.000402             0.000402\n",
      "random                 -0.013      0.075743             -0.00013             -0.00013\n",
      "DDQN (best run)         0.0254     0.0751188             0.000254             0.000254\n",
      "DDQN (mean)             0.014      0.0967264             0.00014              0.00014\n"
     ]
    }
   ],
   "source": [
    "f = open(f\"{outdir}image_folder/table_benchmarking\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To witness how the mean strategy and the individual strategies act see the figures below. They shows the average inventory, cash and value process of the different strategies when evaluted for *n\\_test* episodes.\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/visualization_mean.png\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "    <img src=\"results/mc_model_deep/mc_deep_example/image_folder/visualization_all.png\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83f73d5875a575e504ba23451a5997fea59c0c75034f677431fe9f5bc2b0207e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
